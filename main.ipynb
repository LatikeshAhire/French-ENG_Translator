{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-build-an-encoder-decoder-translation-model-using-lstm-with-python-and-keras-a31e9d864b9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./fra-eng/fra.txt','r',encoding='utf-8') as f:\n",
    "    raw_data=f.read()\n",
    "\n",
    "raw_data = raw_data.split('\\n')\n",
    "pairs = [sentence.split('\\t') for sentence in  raw_data]\n",
    "\n",
    "pairs = pairs[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent):\n",
    "    sent=sent.lower()\n",
    "    \n",
    "    punc=string.punctuation + \"¡\" + '¿'\n",
    "    sent=sent.translate(str.maketrans('','',punc))\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    text_tokenizer = Tokenizer()\n",
    "    text_tokenizer.fit_on_texts(sent)\n",
    "    # print(text_tokenizer.word_index.items())\n",
    "    # print(text_tokenizer.texts_to_sequences(sent))\n",
    "    return text_tokenizer.texts_to_sequences(sent), text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('go', 1), ('va', 2), ('cc', 3), ('by', 4), ('2', 5), ('0', 6), ('france', 7), ('attribution', 8), ('tatoeba', 9), ('org', 10), ('2877272', 11), ('cm', 12), ('1158250', 13), ('wittydev', 14)])\n",
      "14\n",
      "[[1], [2], [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]]\n"
     ]
    }
   ],
   "source": [
    "text_tokenizer=Tokenizer()  \n",
    "text_tokenizer.fit_on_texts(pairs[0])\n",
    "print(text_tokenizer.word_index.items())\n",
    "print(len(text_tokenizer.word_index))\n",
    "print(text_tokenizer.texts_to_sequences(pairs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length English sentence: 9\n",
      "Maximum length French sentence: 14\n",
      "French vocabulary is of 21525 unique words\n",
      "English vocabulary is of 9071 unique words\n"
     ]
    }
   ],
   "source": [
    "english_sentences = [clean_sentence(pair[0]) for pair in pairs]\n",
    "french_sentences = [clean_sentence(pair[1]) for pair in pairs]\n",
    "\n",
    "eng_text_tokenized, eng_text_tokenizer = tokenize([clean_sentence(pair[0]) for pair in pairs])\n",
    "fre_text_tokenized, fre_text_tokenizer = tokenize([clean_sentence(pair[1]) for pair in pairs])\n",
    "\n",
    "max_eng_len=len(max(eng_text_tokenized,key=len))\n",
    "max_fre_len=len(max(fre_text_tokenized,key=len))\n",
    "\n",
    "print('Maximum length English sentence: {}'.format(max_eng_len))\n",
    "print('Maximum length French sentence: {}'.format(max_fre_len))\n",
    "\n",
    "french_vocab = len(fre_text_tokenizer.word_index) + 1\n",
    "english_vocab = len(eng_text_tokenizer.word_index) + 1\n",
    "print(\"French vocabulary is of {} unique words\".format(french_vocab))\n",
    "print(\"English vocabulary is of {} unique words\".format(english_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_pad_sentence=pad_sequences(fre_text_tokenized,max_fre_len,padding='post')\n",
    "eng_pad_sentence=pad_sequences(eng_text_tokenized,max_eng_len,padding='post')\n",
    "\n",
    "# fre_pad_seq[2150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_pad_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)\n",
    "fre_pad_sentence = fre_pad_sentence.reshape(*fre_pad_sentence.shape, 1)\n",
    "\n",
    "# eng_pad_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq=Input(shape=max_fre_len,)\n",
    "embedding=Embedding(input_dim=french_vocab,output_dim=128)(input_seq)    #will create an array of dim(english vocab size,128)  128 is the number of features\n",
    "encoder=LSTM(64,return_sequences=False)(embedding)      #Even though each time step of the LSTM outputs a hidden vector we dont need it for encoder so return_seq is false\n",
    "r_vec=RepeatVector(max_eng_len)(encoder)\n",
    "decoder=LSTM(64,return_sequences=True,dropout=0.2)(r_vec)\n",
    "logits=TimeDistributed(Dense(english_vocab))(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "As we can see in the image the hidden vector is repeated n times, so each time step of the LSTM receives the same vector (decoder)<br>\n",
    "In order to have this same vector for every time step we need to use the layer RepeatVector, as its names implies its role is to repeat the vector it is receiving, the only parameter we need to define is n, the number of repetitions.<br>\n",
    "This number is equal to the number of time step of the decoder part, in other words the maximum English sentence length, 6.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Once we have the input ready, we will continue with the decoder. This is also built with a LSTM layer, the difference is the parameter return_sequences, which in this case is ‘True’.<br> \n",
    "What is this parameter for? In the encoder part we were expecting only one vector in the last time step and neglecting all the others, here we are expecting an output vector at every time step so the Dense layer can make a prediction.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">We have just seen how to apply the Dense layer and predict one word, but how do we make the prediction for the whole sentence? Because we are using return_sequence=True, LSTM layer outputs a vector at every time step, so we need to apply the previous explained Dense layer at every time step and predict one word at a time. To do this, Keras has developed a specific layer called TimeDistributed, it applies the same Dense layer to every time step.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 14)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 14, 128)           2755200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 9, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 9, 64)             33024     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 9, 9071)           589615    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9, 9071)           0         \n",
      "=================================================================\n",
      "Total params: 3,427,247\n",
      "Trainable params: 3,427,247\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Model(input_seq,Activation('softmax')(logits))\n",
    "model.compile(optimizer=Adam(1e-3),\n",
    "              loss=sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 291s 92ms/step - loss: 3.2603 - accuracy: 0.5317\n",
      "Epoch 2/10\n",
      " 569/3125 [====>.........................] - ETA: 4:04 - loss: 3.0519 - accuracy: 0.5378"
     ]
    }
   ],
   "source": [
    "result=model.fit(fre_pad_sentence,eng_pad_sentence, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11, 7749)\n",
      "(11, 7749)\n",
      "[[3.6618917e-03 1.5258732e-04 9.6696804e-06 ... 1.4072783e-06\n",
      "  1.5732898e-06 5.1300553e-06]\n",
      " [5.7763761e-01 8.2470177e-07 3.9365095e-05 ... 6.7855808e-08\n",
      "  6.8940295e-08 1.6914657e-07]\n",
      " [8.2410157e-01 2.3936357e-07 3.3217275e-06 ... 4.4366232e-08\n",
      "  4.2616811e-08 2.5443842e-07]\n",
      " ...\n",
      " [9.9857342e-01 3.8039929e-07 4.5467485e-05 ... 1.5797486e-10\n",
      "  1.5894412e-10 2.8378189e-09]\n",
      " [9.9867356e-01 3.6835235e-07 4.3847383e-05 ... 1.4109483e-10\n",
      "  1.4133751e-10 2.6176972e-09]\n",
      " [9.9873930e-01 3.6272976e-07 4.1970387e-05 ... 1.3185808e-10\n",
      "  1.3160180e-10 2.4757236e-09]]\n"
     ]
    }
   ],
   "source": [
    "print((model.predict(fre_pad_sentence[14:15]).shape))\n",
    "print((model.predict(fre_pad_sentence[14:15])[0].shape))\n",
    "print((model.predict(fre_pad_sentence[14:15])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fre_pad_sentence[14] is a 2d array but we need to pass 3d array to the model so pass fre_pad_sentence[14:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English sentence is: thanks\n",
      "The French sentence is: merci \n",
      "The predicted sentence is :\n",
      "restez          \n"
     ]
    }
   ],
   "source": [
    "def logits_to_sentence(logits, tokenizer):\n",
    "\n",
    "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '' \n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "index = 135\n",
    "print(\"The english sentence is: {}\".format(english_sentences[index]))\n",
    "print(\"The french sentence is: {}\".format(french_sentences[index]))\n",
    "print('The predicted sentence is :')\n",
    "print(logits_to_sentence(model.predict(fre_pad_sentence[index:index+1])[0], eng_text_tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model,open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
